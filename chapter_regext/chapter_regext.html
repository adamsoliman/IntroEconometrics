<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>ECON 4050: Introduction to Econometrics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Adam Soliman, PhD" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <script src="https://use.fontawesome.com/5235085b15.js"></script>
    <link rel="stylesheet" href="../css/scpo-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# ECON 4050: Introduction to Econometrics
]
.subtitle[
## Linear Regression Extensions
]
.author[
### Adam Soliman, PhD
]
.date[
### Clemson University
]

---






# Today - Linear Regression Extensions

Depending on the data and the relationships between the variables of interest, you may need to move away from the baseline model.

--

We will focus on 3 important variations:
  
  1. ***Non-linear relationships***: log and polynomial models
  
  1. ***Interactions*** between variables

  1. ***Standardized*** regression (will not be covered, see appendix if interested)

--

In each case, the way we estimate these coefficients does not change (i.e OLS).

--

Empirical applications:

  * *college tuition* and *earnings potential*,
  * *wage*, *education* and *gender*,
  * *class size* and *student performance*

---

layout: false
class: title-slide-section-red, middle

# Non-Linear Relationships

---

# Accounting for Non-Linear Relationships

There are two main "methods":

--

1. ***Log*** models

1. ***Polynomial*** models

---


# Log Models

* The models we have seen so far can be called ***level-level*** specifications. Both the dependent and the independent variables have been measured in levels.

--

  * The *level* can be dollars, years, number of students...even a percentage.
  
--

* Taking the *natural* logarithm of the dependent and/or the independent variable(s) leads us to define 3 other types of regressions (with a common abuse of notation where `\(\ln(x) = \log_{e}(x)=\log(x)\)`):

  * ***Log - level***: `\(\quad \log(y_i) = b_0 + b_1 x_{1,i} + ... + e_i\)`

  * ***Level - log***: `\(\quad \textrm{y}_i = b_0 + b_1 \log(x_{1,i}) + ... + e_i\)`

  * ***Log - log***: `\(\quad \log(y_i) = b_0 + b_1 \log(x_{1,i}) + ... + e_i\)`
  
---

# The (natural) log Function: A Primer üòâ

&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-1-1.svg" style="display: block; margin: auto;" /&gt;

---

# The (natural) log Function: A Primer üòâ

* The [natural log function](https://en.wikipedia.org/wiki/Natural_logarithm) is the inverse function of the exponential function, i.e. `\(\log(\exp(x))=x\)`

* Since for all `\(x\)`, `\(\exp(x)&gt;0 \implies\)` natural log function is only defined for ***strictly positive values***! (It is not defined in 0!)

‚ö†Ô∏è Note you can only log your variables if they don't take 0 or negative values! Always think about this when taking the log of your dependent or independent variable(s)

---

# The (natural) log Function: A Primer üòâ

If you have very ***skewed distributions*** taking the log will render it more ***normally distributed***

--

.pull-left[
&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[
&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;
]


---

# Log Models: Simplified Interpretations

|    Specification           | Model  |  Interpretation of `\(b_1\)`            |
|--------------------|:---------:|:-----------------------------------:|
| Level - Level | `\(y = b_0 + b_1 x + e\)` | .small[A **one unit** increase in ] `\(x\)` .small[ is associated, on average, with a ] `\(b_1\)` .small[**unit change** in y]  |
| Log - Level | `\(\log(y) = b_0 + b_1 x + e\)` | .small[A **one unit** increase in ] `\(x\)` .small[ is associated, on average, with a] `\(b_1 \times 100\)` .small[ **percent change** in y]  |
| Level - Log | `\(y = b_0 + b_1 \log(x)  + e\)` | .small[A **one percent** increase in ] `\(x\)` .small[ is associated, on average, with a ] `\(b_1 / 100\)`  .small[**unit change** in y] |
| Log - Log  |  `\(\log(y) = b_0 + b_1 \log(x) + e\)` | .small[A **one percent** increase in ] `\(x\)` .small[ is associated, on average, with a] `\(b_1\)` .small[**percent change** in y]  |

* This may look like cooking recipes but of course it can be [derived with some relatively simple math](https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faqhow-do-i-interpret-a-regression-model-when-some-variables-are-log-transformed/).

* ‚ö†Ô∏è these interpretations are only true for ***small*** changes in `\(x\)` and small/or `\(b_1\)`. What happens if we want to know the change in `\(y\)` for big changes in `\(x\)` or when `\(b_1\)` is large?

---
name: gen_log

# Log Models: General Interpretations

For ***any increase in `\(x\)`, `\(\Delta x,\)` and any `\(b_1\)`*** `\((\Delta x = 5\% = 0.05 \implies 1 + \Delta x = 1.05)\)`:

|    Specification           | Model  |  Interpretation of `\(b_1\)`            |
|--------------------|:---------:|:-----------------------------------:|
| Level - Level | `\(y = b_0 + b_1 x + e\)` | .small[A **one unit** increase in ] `\(x\)` .small[ is associated, on average, with a ] `\(b_1\)` .small[**unit change** in y]  |
| Log - Level | `\(\log(y) = b_0 + b_1 x + e\)` | .small[A **one unit** increase in ] `\(x\)` .small[ is associated, on average, with a] `\((e^{b_1} - 1) \times 100\)` .small[ **percent change** in y]  |
| Level - Log | `\(y = b_0 + b_1 \log(x)  + e\)` | .small[A  ] ** `\(\Delta x\)`** .small[**percent** increase in ] `\(x\)` .small[ is associated, on average, with a ] `\(b_1 \times \log(1 + \Delta x)\)`  .small[**unit change** in y] |
| Log - Log  |  `\(\log(y) = b_0 + b_1 \log(x) + e\)` | .small[A  ] ** `\(\Delta x\)`** .small[**percent** increase in ] `\(x\)` .small[ is associated, on average, with a] `\(((1 + \Delta x)^{b_1} - 1) \times 100\)` .small[**percent change** in y]  |


([*Appendix:*](#log_approx) Why are the previously shown approximations true?)


---

# When Should You Use log Models?

1. If the relationship betwen `\(x\)` and `\(y\)` looks like a log or exponential function

--

.pull-left[
&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[
&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-5-1.svg" style="display: block; margin: auto;" /&gt;
]

---

# When Should You Use log Models?

1. If the relationship betwen `\(x\)` and `\(y\)` looks like a log or exponential function

.pull-left[
&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-6-1.svg" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[
&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-7-1.svg" style="display: block; margin: auto;" /&gt;
]

---

# When Should You Use log Models?

1. If the relationship betwen `\(x\)` and `\(y\)` looks like a log or exponential function. 

1. To easily interpret coefficients as &lt;a href="https://en.wikipedia.org/wiki/Elasticity_(economics)"&gt;***elasticities***&lt;/a&gt; which play a central role in economic theory

***Elasticity of `\(y\)` with respect to `\(x\)`:*** percent change in `\(y\)` following a 1% increase in `\(x\)`.

---

# Accounting for Other Types Non-Linear Relationships

What if the relationship between `\(x\)` and `\(y\)` is not exponential/log?

`\(\rightarrow\)` ***polynomial*** regressions: just take a polynomial function of the regressor!

---

# Polynomial Wut? üòü

.pull-left[
&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-8-1.svg" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-9-1.svg" style="display: block; margin: auto;" /&gt;
]

---

# Polynomial Regressions

What does this mean in practice?

`\(\rightarrow\)` add a higher order of the regressor to the regression, depending on the visual (or expected) relationship

--

Several ways of doing this in `R`, these are just two of the equivalent ones:

.pull-left[

```r
lm(y ~ x + I(x^2) + I(x^3), data)
```
]

.pull-right[

```r
lm(y ~ poly(x, 3, raw = TRUE), data)
```
]


---

# Polynomial Regressions


.pull-left[

***2nd order:***

&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-12-1.svg" style="display: block; margin: auto;" /&gt;
]

--

.pull-left[

***3rd order:***

&lt;img src="chapter_regext_files/figure-html/unnamed-chunk-13-1.svg" style="display: block; margin: auto;" /&gt;
]

---

&lt;img src="chapter_regext_files/figure-html/curve_fitting.png" width="378px" height="600px" style="display: block; margin: auto;" /&gt;

---

# Task 1: Non-linear relationships

1. Load the data [here](https://www.dropbox.com/s/2v5mb04nzw2u7bd/college_tuition_income.csv?dl=1). This dataset contains information about tuition and estimated incomes of graduates for universities in the US. More details can be found [here](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-03-10/readme.md).

2. Create a scatter plot of estimated mid career pay (`mid_career_pay`) `\((y-axis)\)` as a function of out of state tuition (`out_of_state_tuition`) `\((x-axis)\)`. Would you say the relationship is broadly linear or rather non-linear? Use `geom_smooth(method = "lm", se = F) + geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 2, raw= T))` to fit both a linear and 2nd order regression line. This time which seems most appropriate?

3. Create a variable equal to out of state tuition divided by 1000. Regress mid career pay on out of state tuition divided by 1000. Interpret the coefficient.

4. Regress mid career pay on out of state tuition divided by 1000 and its square. *Hint:* you can use either `poly(x, 2, raw = T)` or `x + I(x^2)`, where x is your regressor. What does the positive sign on the squared term imply?

---

layout: false
class: title-slide-section-red, middle

# Interaction Terms

---

# Interacting Regressors

* In a linear model *without* interaction terms, we assume that the effect of each predictor on the dependent variable (target) is independent of other predictors in the model.

* We interact two regressors when we believe ***the effect of one depends on the value of the other***. 

  * *Example:* The returns to education on wage vary by gender.
  
--
  
* In practice, if we interact `\(x_1\)` and `\(x_2\)`, we would write our model like this : 

`$$y_i =  b_0 + b_1 x_{1,i} + b_2 x_{2,i} + b_3x_{1,i} \times x_{2,i} + ... + e_i$$`

--

* The interpretation of `\(b_1\)`, `\(b_2\)`, and `\(b_3\)` will depend on the type of `\(x_1\)` and `\(x_2\)`.
  
* We will focus on the cases where one regressor is a ***dummy/categorical*** variable and the other is ***continuous***.

* It will give you the intuition for the other cases:

  * Both regresors are dummies/categorical variables,

  * Both regresors are continuous variables.
  
---

# Interacting Regressors

Let's go back to the *STAR* experiment data.

How does the effect of being in a small vs regular class vary with the experience of the teacher?

Our regression model becomes:

$$ \textrm{score}_i = \color{#d96502}{b_0} + \color{#027D83}{b_1} \textrm{small}_i + \color{#02AB0D}{b_2} \textrm{experience}_i + \color{#d90502}{b_3} \textrm{small}_i \times \textrm{experience}_i + e_i$$ 

--

Let's say we estimate it in `R` with real data, and we get the following coefficients for each `\(b\)`:

`$$\hat{score}_i = \color{#d96502}{535} + \color{#027D83}{16} \textrm{small}_i + \color{#02AB0D}{1.3} \textrm{experience}_i + \color{#d90502}{-0.3} \textrm{small}_i \times \textrm{experience}_i$$` 
--

Because `\(\textrm{small}_i\)` is a binary variable, we can split up the previous equation for each class type. 

* For those in regular classes, or when `\(small_i = 0\)`, this would look like:

`$$\hat{score}_i = \color{#d96502}{535} + \color{#02AB0D}{1.3} \textrm{experience}_i,$$` 


* and for those in small classes, or when `\(small_i = 1\)`, we would get:

`$$\hat{score}_i = \color{#d96502}{535} + \color{#027D83}{16} + (\color{#02AB0D}{1.3} - \color{#d90502}{0.3}) \textrm{experience}_i = 551 + \textrm{experience}_i$$` 


---

# Interacting Regressors

Running the regression for the `math` score (for all grades), we obtain:





```r
lm(math ~ small+ experience + small*experience, star_df)
```

```
## 
## Call:
## lm(formula = math ~ small + experience + small * experience, 
##     data = star_df)
## 
## Coefficients:
##          (Intercept)             smallTRUE            experience  
##             534.1919               15.8906                1.3305  
## smallTRUE:experience  
##              -0.3034
```


* The interaction term allows the impact of being in a small class to vary with teacher experience.
  
* In particular, we still observe a ***positive impact of being in a small class*** on math score, but this ***effect is decreasing in the experience of the teacher***.

  
  
---

# Interacting Regressors (more formally)

Running the regression for the `math` score (for all grades), we obtain:



``` r
lm(math ~ small+ experience + small*experience, star_df)
```

```
## 
## Call:
## lm(formula = math ~ small + experience + small * experience, 
##     data = star_df)
## 
## Coefficients:
##          (Intercept)             smallTRUE            experience  
##             534.1919               15.8906                1.3305  
## smallTRUE:experience  
##              -0.3034
```

* `\(\color{#d96502}{b_0}\)` : 534.1919 is the average math score for students assigned to the *regular* classes
* `\(\color{#027D83}{b_1}\)` : 15.8906 is the average increase in math scores for those in a *small* class, relative to those in the regular class
* `\(\color{#02AB0D}{b_2}\)` : 1.3305 means that for those in the *regular* class, average math scores increase by 1.3305 per year of teacher experience (i.e., it is the slope of the regular class line...think when `\(small = 0\)`)
* `\(\color{#02AB0D}{b_2} + \color{#d90502}{b_3}\)` : 1.3305 + -0.3034 = 1.0271 is the slope of the small class line, i.e., that for those in the *small* class, average math scores increase by 1.0271 per year of teacher experience

---

# How is this different than the base case?



``` r
lm(math ~ small+ experience, star_df)
```

```
## 
## Call:
## lm(formula = math ~ small + experience, data = star_df)
## 
## Coefficients:
## (Intercept)    smallTRUE   experience  
##     535.813       12.336        1.188
```

``` r
lm(math ~ small+ experience + small*experience, star_df)
```

```
## 
## Call:
## lm(formula = math ~ small + experience + small * experience, 
##     data = star_df)
## 
## Coefficients:
##          (Intercept)             smallTRUE            experience  
##             534.1919               15.8906                1.3305  
## smallTRUE:experience  
##              -0.3034
```

---

# Interacting Regressors: Visually

$$ \textrm{score}_i = \color{#d96502}{b_0} + \color{#027D83}{b_1} \textrm{small}_i + \color{#02AB0D}{b_2} \textrm{experience}_i + \color{#d90502}{b_3} \textrm{small}_i * \textrm{experience}_i + e_i$$ 



&lt;img src="chapter_regext_files/figure-html/graph_base.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# Interacting Regressors: Visually

$$ \textrm{score}_i = \color{#d96502}{b_0} + \color{#027D83}{b_1} \textrm{small}_i + \color{#02AB0D}{b_2} \textrm{experience}_i + \color{#d90502}{b_3} \textrm{small}_i * \textrm{experience}_i + e_i$$ 



&lt;img src="chapter_regext_files/figure-html/graph_reg.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# Interacting Regressors: Visually

$$ \textrm{score}_i = \color{#d96502}{b_0} + \color{#027D83}{b_1} \textrm{small}_i + \color{#02AB0D}{b_2} \textrm{experience}_i + \color{#d90502}{b_3} \textrm{small}_i * \textrm{experience}_i + e_i$$ 



&lt;img src="chapter_regext_files/figure-html/graph_reg_b0.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# Interacting Regressors: Visually

$$ \textrm{score}_i = \color{#d96502}{b_0} + \color{#027D83}{b_1} \textrm{small}_i + \color{#02AB0D}{b_2} \textrm{experience}_i + \color{#d90502}{b_3} \textrm{small}_i * \textrm{experience}_i + e_i$$ 



&lt;img src="chapter_regext_files/figure-html/graph_reg_b0_b1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# Interacting Regressors: Visually

$$ \textrm{score}_i = \color{#d96502}{b_0} + \color{#027D83}{b_1} \textrm{small}_i + \color{#02AB0D}{b_2} \textrm{experience}_i + \color{#d90502}{b_3} \textrm{small}_i * \textrm{experience}_i + e_i$$ 



&lt;img src="chapter_regext_files/figure-html/graph_reg_b0_b1_b2.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# Interacting Regressors: Visually

$$ \textrm{score}_i = \color{#d96502}{b_0} + \color{#027D83}{b_1} \textrm{small}_i + \color{#02AB0D}{b_2} \textrm{experience}_i + \color{#d90502}{b_3} \textrm{small}_i * \textrm{experience}_i + e_i$$ 



&lt;img src="chapter_regext_files/figure-html/graph_reg_b0_b1_b2_b3.png" width="90%" style="display: block; margin: auto;" /&gt;

---

# Teaser for the Next Few Lectures

* You may have noticed that since the beginning we always work with **samples** drawn from the overall population.


* Each time, imagine we could draw another sample from population:

  * Would we obtain the same results? 
  
  * In other words, how confident can we be that our estimates (sign, magnitude) are not just driven by randomness?

* We will answer those kind of questions:

  * We'll present the notion of **sampling**, and
  
  * Understand what **statistical inference** is and how to do it. 
---

# On the way to causality

‚úÖ How to manage data? Read it, tidy it, visualise it!

‚úÖ  **How to summarise relationships between variables?** Simple and multiple linear regression, non-linear regressions, interactions...

‚úÖ What is causality?

‚ùå What if we don't observe an entire population?

‚ùå  Are our findings just due to randomness?

‚ùå How to find exogeneity in practice?

---
name: log_approx
# Log Models: Approximations

Why are the approximations shown previously true?

--

***Log-Level***

*General interpretation:* A **one unit** increase in `\(x\)` is associated, on average, with a `\((e^{b_1} - 1) \times 100\)` **percent change** in y.

*Simplified interpretation:* A **one unit** increase in `\(x\)` is associated, on average, with a `\(b_1 \times 100\)` **percent change** in y.

--

This is because, for small `\(b_1\)`, `\(e^{b_1} \approx 1+ b_1 \iff b_1 \approx e^{b_1} - 1\)`

--

`\(\rightarrow\)` for `\(b_1 = \color{#d90502}{0.04}\)`, `\(e^{b_1} - 1 = e^{0.04} - 1 = 0.0408\)` 

--

`\(\rightarrow\)` for `\(b_1 = \color{#d90502}{0.5}\)`, `\(e^{b_1} - 1 = e^{0.5} - 1 = 0.6487\)`

---

# Log Models: Approximations

Why are the approximations shown previously true?

***Level-Log***

*General interpretation:* A ** `\(\Delta x\)`** **percent** increase in `\(x\)` is associated, on average, with a `\(b_1 \times log(1 + \Delta x)\)` **unit change** in y.

*Simplified interpretation:* A **one percent** increase in `\(x\)` is associated, on average, with a `\(b_1 / 100\)` **unit change** in y.

--

This is because for small `\(\Delta x\)`, `\(log(1 + \Delta x) \approx \Delta x\)`

--

`\(\rightarrow\)` for `\(\Delta x = \color{#d90502}{1\%}=0.01\)`, `\(log(1+\Delta x) = log(1.01) = 0.01\)` (hence the `\(/100\)` in the simplified interpretation)

--

`\(\rightarrow\)` for `\(\Delta x = \color{#d90502}{20\%}=0.20\)`, `\(log(1+\Delta x) = log(1.20) = 0.18\)`

---

# Log Models: Approximations

Why are the approximations shown previously true?

***Log-Log***

*General interpretation:* A ** `\(\Delta x\)`** **percent** increase in `\(x\)` is associated, on average, with a `\(((1 + \Delta x)^{b_1} - 1) \times 100\)` **percent change** in y.

*Simplified interpretation:* A **one percent** increase in `\(x\)` is associated, on average, with a `\(b_1\)` **percent change** in y.

--

This is because for small `\(|b_1|\times \Delta x\)`, `\((1 + \Delta x)^{b_1} \approx 1 + b_1 \times \Delta x \iff b_1 \times \Delta x \times 100 \approx ((1 + \Delta x)^{b_1} - 1) \times 100\)`

--

`\(\rightarrow\)` for `\(\Delta x = \color{#d90502}{1\%}=0.01\)` and `\(b_1 = \color{#d90502}{0.5}\)`, `\(((1+\Delta x)^{b_1} - 1) \times 100 = (1.01^{0.5} - 1) \times 100 = 0.5\)`

--

`\(\rightarrow\)` for `\(\Delta x = \color{#d90502}{10\%}=0.10\)` and `\(b_1 = \color{#d90502}{10}\)`, `\(((1+\Delta x)^{b_1} - 1) \times 100= (1.1^{10} - 1) \times 100 = 159.37\)`


[back](#gen_log)


---

# Standardized Regression

Let's define what *standardizing* a variable means.

&gt; ***Standardizing*** a variable `\(z\)`  means to *demean* the variable and to divide the demeaned value by its own standard deviation:

$$ z_i^{stand} = \frac{z_i - \bar z}{\sigma(z)}$$ 
where `\(\bar z\)` is the mean of `\(z\)` and `\(\sigma(z)\)` is the standard deviation of `\(z\)`, i.e. `\(\sigma(z) = \sqrt{\textrm{Var}(z)}\)`.

--

`\(z^{stand}\)` now has mean 0 and standard deviation 1, i.e. `\(\overline{z^{stand}} = 0\)` and `\(\sigma(z^{stand}) = 1\)`

--

Intuitively, standardizing ***puts variables on the same scale*** so we can compare them.
  
In our class size and student performance example, it will help to interpret: 

  * The **magnitude** of the effects
  * The **relative importance of each variable**

---

# Standardized Regression: Graphically



.pull-left[
&lt;img src="chapter_regext_files/figure-html/graph_before.png" width="3200" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[
&lt;img src="chapter_regext_files/figure-html/graph_after.png" width="3200" style="display: block; margin: auto;" /&gt;
]

---

# Standardized Regression: Interpretation

If the ***dependent*** variable `\(y\)` is standardized, i.e. the model is `\(\color{#d90502}{y^{stand}} = b_0 + \sum_{k=1}^Kb_kx_k +e\)`:

--

* By definition, `\(b_k\)` measures the predicted change in ** `\(y^{stand}\)` **  associated with a one unit increase in `\(x_k\)`.
  
* If `\(y^{stand}\)` increases by one, it means that `\(y\)` increases by one standard deviation. So `\(b_k\)` measures the change in `\(y\)` **as a share of `\(y\)`'s standard deviation**.
  
--

If the ***regressor*** `\(x_k\)` is standardized, i.e. the model is `\(y = b_0 + \sum_{k=1}^Kb_k\color{#d90502}{x_k^{stand}} +e\)`:

--

* By definition, `\(b_k\)` measures the predicted change in `\(y\)` associated with a one unit increase in ** `\(x_k^{stand}\)` **. 
  
* If `\(x_k^{stand}\)` increases by one unit, it means that `\(x_k\)` increases by one standard deviation. So `\(b_k\)` measures the predicted change in `\(y\)` **associated with an increase in `\(x_k\)` by one standard deviation**. 
  
---

# Task 2: Standardized regression

Let's go back our [grades](https://www.dropbox.com/scl/fi/77u2r8xznh63jg7aisq4f/grade5_isl.csv?rlkey=uclo7ng24so9fct41kcjm8k0l&amp;dl=0) dataset. Please download it from the previous link and read it in using the appropriate function. Below are the estimates we got from regressing average math test scores on the full set of regressors.


```
##       (Intercept)          classize     disadvantaged school_enrollment 
##      78.560725298       0.003320773      -0.389333008       0.000758258 
##            female         religious 
##       0.923710499       2.876146701
```

1. Create a new variable `avgmath_stand` equal to the standardized math score. You can use the `scale()` function (combined with `mutate`) or do it by hand with base `R`.

1. Run the full regression using the standardized math test score as the dependent variable. Interpret the coefficients and their magnitude.

1. Create the standardized variables for each *continuous* regressor as `&lt;regressor&gt;_stand`.
  * Would it make sense to standardize the `religious` variable?

1. Regress `avgmath_stand` on the full set of standardized regressors and `religious`. Discuss the relative influence of the regressors.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://cdnjs.cloudflare.com/ajax/libs/remark/0.14.0/remark.min.js"></script>
<script src="../js/ru_xaringan.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
